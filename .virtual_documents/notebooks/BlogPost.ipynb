get_ipython().run_line_magic("load_ext", " jupyter_black")
get_ipython().run_line_magic("load_ext", " autoreload")
get_ipython().run_line_magic("autoreload", " 2")


# Import required libraries
from leafsim import LeafSim
import matplotlib.pylab as plt
import matplotlib.ticker as ticker
import pandas as pd
import numpy as np
from IPython.display import Image
import seaborn as sns
from sklearn.model_selection import train_test_split
import shap
from catboost import CatBoostRegressor

# Helper functions
from utils import get_similarity_table, get_similarity_plots
from IPython.display import display


# https://github.com/catboost/catboost/issues/2179
import warnings

warnings.simplefilter(action="ignore", category=FutureWarning)


# Define settings
plt.rcParams["figure.dpi"] = 400
font = {"family": "serif", "weight": "normal"}
plt.rc("font", **font)
plt.rc("xtick", labelsize=16)
plt.rc("ytick", labelsize=16)
plt.rc("axes", labelsize=20)
plt.rc("figure", titlesize=22)
plt.rc("legend", fontsize=14)
np.random.seed(46)
shap.initjs()


def load_df(filepath):
    df = pd.read_csv(filepath)
    # Clean column names
    df.columns = list(
        map(
            lambda x:
            x.strip().lower().replace(" ", "").replace("(£)", ""),
            df.columns
        )
    )
    # Add brand (taken from filename)
    df["brand"] = filepath.split("/")[-1].replace(".csv", "")
    return df


# Manually specify files instead of using
# glob.glob("../data/[!unclean]*.csv")
# so that files are read in same order across systems.
# In the future, use sorted() but this would affect
# the progress made so far in the notebook
# as results would change
files = [
    "../data/vauxhall.csv",
    "../data/bmw.csv",
    "../data/vw.csv",
    "../data/hyundi.csv",
    "../data/toyota.csv",
    "../data/ford.csv",
    "../data/focus.csv",
    "../data/skoda.csv",
    "../data/merc.csv",
]
df = pd.concat(
    [
        # Disregard files whose name starts with "unclean"
        load_df(i)
        for i in files
    ]
)
df.reset_index(drop=True, inplace=True)


(
    df
    .dtypes
    .to_frame()
    .reset_index()
    .rename(
        columns={
            "index": "column",
            0: "data_type"
        }
    )
)


# Define features
feature_cols = [
    "brand",
    "model",
    "year",
    "mileage",
    "transmission",
    "fueltype",
    "enginesize",
]
# of which the following ones are of categorical type:
categorical_feature_cols = ["brand", "model", "fueltype", "transmission"]

numeric_cols = [i for i in feature_cols if i not in categorical_feature_cols]

# Define the target of the model, i.e. what we want to predict
target_col = "price"


df = (
    df.loc[lambda x: (x.enginesize > 0) & (x.year <= 2022)]
    .copy()
    .reset_index(drop=True)
)


# Create an ID to indexing precisely if random seed differs between systems
df["ID"] = df.index.tolist()


fig, axes = plt.subplots(4, (len(feature_cols) + 1) // 4, figsize=(20, 20))
plt.suptitle("Car attributes considered", fontsize=20)
axes = axes.flatten()
for i, c in enumerate(feature_cols + [target_col]):
    if c in categorical_feature_cols:
        if df[c].nunique() > 15:
            tmp = df[c].value_counts(normalize=True).reset_index()
            tmp.loc[15:, "index"] = "Other"
            tmp = tmp.groupby("index")[c].sum().sort_values(ascending=False)
            tmp.plot(kind="bar", ax=axes[i], color="darkviolet", alpha=0.7)
            axes[i].set_xlabel("Top 15 + all remaining in 'Other'")
        else:
            df[c].value_counts(normalize=True).plot(
                kind="bar", ax=axes[i], color="darkviolet", alpha=0.7
            )
    else:
        sns.histplot(
            data=df, x=c, ax=axes[i],
            stat="probability", color="darkviolet",
            alpha=0.7
        )
        axes[i].axvline(
            df[c].mean(),
            label="Average",
            linewidth=1,
            linestyle="--",
            color="royalblue",
        )
        axes[i].set_xlabel("")
        axes[i].legend()
    axes[i].set_title(c.title(), fontsize=22)
    axes[i].set_ylabel("Share")
    axes[i].yaxis.set_major_formatter(
        ticker.FuncFormatter(lambda x, pos: f"{x*100:,.0f}%")
    )
    if c not in ["year", "model"]:
        axes[i].xaxis.set_major_formatter(
            ticker.FuncFormatter(lambda x, pos: f"{x:,.0f}")
        )
    if c == "price":
        axes[i].xaxis.set_major_formatter(
            ticker.FuncFormatter(lambda x, pos: f"£{x:,.0f}")
        )
        axes[i].tick_params(axis="x", labelrotation=45)
plt.tight_layout()
plt.savefig("../resources/eda_plot.png", dpi=600)


# Initiate model
model = CatBoostRegressor(random_seed=46, n_estimators=50, min_data_in_leaf=4)

# Define name of prediction column
predicted_col = "predicted_" + target_col

# Set categorical features to categorical dtype
df[categorical_feature_cols] = (
    df[categorical_feature_cols].fillna("None").astype("category")
)

# Define splits
train_idx, test_idx = train_test_split(
    np.arange(len(df)), test_size=0.2, random_state=46
)

# Manually modify the splits because there are 2 cars
# we would like to have in our test set
# As these will be used as examples throughout the notebook
# In case the random seed has a different
# effect across platforms and machines
# we can make sure that at least these 2 examples are the same
car_ids_in_test = [20844, 13284]
for car_id in car_ids_in_test:
    if car_id not in test_idx:
        print(f"Modifying train/test split to include car ID {car_id}")
        test_idx = np.append(test_idx, car_id)
        car_idx_train = np.where(train_idx == car_id)[0][0]
        train_idx = np.delete(train_idx, car_idx_train)
assert all([i in test_idx and i not in train_idx for i in car_ids_in_test])

# Create data splits
df_train = df.loc[train_idx].reset_index(drop=True).copy()
df_test = df.loc[test_idx].reset_index(drop=True).copy()


# Train model
model.fit(
    df_train[feature_cols],
    df_train[target_col],
    cat_features=categorical_feature_cols,
    verbose=False,
);


# Make predictions
for ds in [df_train, df_test]:
    # Make predictions
    ds[predicted_col] = model.predict(ds[feature_cols]).astype(int)
    # Evaluate model
    diff = ds[predicted_col] - ds[target_col]
    ds["absolute_percentage_error"] = diff.abs() / ds[target_col]
    ds["relative_percentage_error"] = diff / ds[target_col]


(
    pd.concat((df_train.assign(split="train"), df_test.assign(split="test")))
    .groupby("split")
    .agg(
        mepe=("relative_percentage_error", "mean"),
        mape=("absolute_percentage_error", "mean"),
    )
    .rename(
        columns={
            "mepe": "Mean Relative Percentage Error",
            "mape": "Mean Absolute Percentage Error",
        }
    )
    .applymap(lambda x: "{:.1f}%".format(x * 100))
)


model_explainer = shap.TreeExplainer(model)
shap_values = model_explainer(df_train[feature_cols])
shap_values_test = model_explainer(df_test[feature_cols])


shap.plots.bar(shap_values)


shap.plots.beeswarm(shap_values)


# Pick a specific car with nice properties
# for the sake of explaining how SHAP works
car_to_explain_id = 20844
example = df_test.query(f"ID == {car_to_explain_id}")[feature_cols]
car_to_explain_idx = example.index[0]
display(example)


shap.plots.waterfall(shap_values_test[car_to_explain_idx])


# Use Image to display so it can be rendered in PDF when running
# jupyter nbconvert --to webpdf --no-input notebook.ipynb
# As opposed to:
# <div>
# <img src="images/decision_tree_regression.jpg" width="600"/>
# </div>
Image(filename="../resources/decision_tree_regression.jpg", width=600)


Image(filename="../resources/leaf_index.png", width=1200)


# Since this notebook is available on a hosting platform
# and can be executed there, let's reduce the computational complexity
# by only finding similar training observations for some cars in the test set
df_test_subset = pd.concat(
    (
        df_test.loc[lambda x: ~x.ID.isin(car_ids_in_test)].sample(frac=0.2),
        df_test.loc[lambda x: x.ID.isin(car_ids_in_test)],
    )
).reset_index(drop=True)


leaf_sim = LeafSim(model)


# Top 10
top_n_ids, top_n_similarities = leaf_sim.generate_explanations(
    X_train=df_train[feature_cols].values,
    X_to_explain=df_test_subset[feature_cols].values,
    top_n=10,
)


# Top 50 (for more advanced analysis)
_, top_50_similarities, similarities = leaf_sim.generate_explanations(
    X_train=df_train[feature_cols].values,
    X_to_explain=df_test_subset[feature_cols].values,
    top_n=50,
    return_all_similarities=True,
)


# Get the most important features according to SHAP as a list
# These features will be used when showing similar instances
# Measure importance in absolute terms
abs_mean_shap_values = np.abs(shap_values.values).mean(0)
df_feature_importance = pd.DataFrame(
    list(zip(feature_cols, abs_mean_shap_values)),
    columns=["feature_name", "feature_importance"],
).sort_values(by=["feature_importance"], ascending=False)
# Restrict ourselves to the top 8 most influential features
top_n_features = list(df_feature_importance.feature_name.values[:8])

# Define the formatting of some columns
formatting = {
    target_col: lambda x: f"£ {x:,.0f}",
    predicted_col: lambda x: f"£ {x:,.0f}",
    "similarity": lambda x: f"{x*100:.0f}%",
    "enginesize": lambda x: f"{x:.1f}",
}


avg_top_sim = top_50_similarities.mean(axis=1)
df_test_subset["avg_top_sim"] = avg_top_sim


# Wrap the utils function into another function to
# reduce the number of parameters we have to pass
_get_similarity_table = lambda car_id: get_similarity_table(
    df_train,
    df_test_subset,
    top_n_ids,
    top_n_similarities,
    car_id,
    top_n_features,
    formatting,
)

_get_similarity_plots = lambda car_id: get_similarity_plots(
    df_train, df_test_subset, similarities, car_id, avg_top_sim
)


# Pick the same car from the SHAP example
car_to_explain_id = 20844  # index in df_test
example = df_test_subset.query(f"ID == {car_to_explain_id}")[
    top_n_features + [predicted_col, target_col]
]
car_to_explain_idx = example.index[0]


# Will not be nicely rendered in Quarto
# See https://github.com/quarto-dev/quarto-cli/discussions/1716
# display(example.style.format(formatting))
# Hence don't format prices:
(
    example.assign(
        predicted_price=lambda x: x.predicted_price.apply("£{:,}".format),
        price=lambda x: x.price.apply("£{:,}".format),
    )
)


display(_get_similarity_table(car_to_explain_idx))


_get_similarity_plots(car_to_explain_idx)
plt.savefig("../resources/example_1.png", dpi=600)


# Test set error rate by fueltype
(
    df_test.groupby(["fueltype"])
    .agg(mean_absolute_percentage_error=("absolute_percentage_error", "mean"))
    .sort_values(by="mean_absolute_percentage_error")
    .rename(
        columns={
            "mean_absolute_percentage_error": "Mean Absolute Percentage Error"
        }
    )
    .applymap(lambda x: "{:.1f}%".format(x * 100))
)


# Pick an electric car
car_to_explain_id = 13284  # index in df_test
example = df_test_subset.query(f"ID == {car_to_explain_id}")[
    top_n_features + [predicted_col, target_col]
]
car_to_explain_idx = example.index[0]


# display(example.style.format(formatting))
(
    example.assign(
        predicted_price=lambda x: x.predicted_price.apply("£{:,}".format),
        price=lambda x: x.price.apply("£{:,}".format),
    )
)


display(_get_similarity_table(car_to_explain_idx))


_get_similarity_plots(car_to_explain_idx)
plt.savefig("../resources/example_2.png", dpi=600)



