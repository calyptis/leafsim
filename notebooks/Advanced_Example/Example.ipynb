{
 "cells": [
  {
   "cell_type": "raw",
   "id": "13d231d0-9044-4607-ac91-83030a46aba0",
   "metadata": {},
   "source": [
    "---\n",
    "title-block-banner: ../images/Banner.png\n",
    "title-block-banner-color: black\n",
    "title: \"LeafSim\"\n",
    "subtitle: \"An Example-Based XAI for Decision Tree Based Ensemble Methods\"\n",
    "author: \"Lucas Chizzali\"\n",
    "date: 2022-12-16\n",
    "toc-title: \"Table of Contents\"\n",
    "toc: true\n",
    "number-sections: true\n",
    "format:\n",
    "  html:\n",
    "    code-fold: true\n",
    "    fig-format: svg\n",
    "    fig-dpi: 400\n",
    "embed-resources: true\n",
    "tok: true\n",
    "tbl-colwidths: auto\n",
    "self-contained-math: true\n",
    "theme: cosmo\n",
    "page-layout: full\n",
    "execute:\n",
    "    enabled: false\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d26a52-0991-483f-bff8-4ed7664c918b",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "This blog presents `LeafSim`, an example-based **explainable AI** (**XAI**) technique for decision tree based ensemble methods.<br/>\n",
    "The process applies the Hamming distance on leaf indices to measure similarity between instances in the test and training set.<br/>\n",
    "It therefore explains model predictions by identifying training data points that most influenced a given prediction.\n",
    "\n",
    "The proposed technique is:\n",
    "\n",
    "- easy to interpret by non-technical people\n",
    "- complementing existing XAI techniques\n",
    "- straightforward to implement & maintain in production\n",
    "- computationally lightweight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3529fdf9-632f-40ea-96ec-7466a36b5dcd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Introduction\n",
    "\n",
    "This blog post will explore example-based XAI techniques. As such, the article will introduce an approach that aims to complement well-established techniques, such as [SHAP](https://github.com/slundberg/shap) and [Lime](https://github.com/marcotcr/lime).<br/>\n",
    "Specifically, the approach aims at explaining individual predictions of decision tree based ensemble methods, such as [Catboost](https://github.com/catboost/catboost), regardless of whether they are regressors or classifiers.\n",
    "\n",
    "Existing XAI approaches, such as SHAP, provide insights into the most relevant features, both from a global and local perspective. In addition, they can provide perspective to predictions of individual data points, such as measuring how model predictions differ with varying feature values.\n",
    "\n",
    "However, for people with limited knowledge in Machine Learning, it can be less intuitive to parse such information and build an understanding of what underpins model predictions.\n",
    "\n",
    "In such cases, providing example-based explanations can be very helpful, as we outline in this blog post."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cd5c10-3084-4d3a-8b6e-136eca495583",
   "metadata": {},
   "source": [
    "# Coding Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedf0c10-a33b-4e42-b75b-130a882b6191",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext jupyter_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f29cbc-648a-4d57-9583-7718bb6abbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "# from leafsim import LeafSim\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import Image\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shap\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# Helper functions\n",
    "from utils import get_similarity_table, get_similarity_plots\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a637dee8-f7f2-4c72-8f7a-8488aa244f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/catboost/catboost/issues/2179\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42613289-a0b9-4e40-b988-4da4ac177cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define settings\n",
    "plt.rcParams[\"figure.dpi\"] = 400\n",
    "font = {\"family\": \"serif\", \"weight\": \"normal\"}\n",
    "plt.rc(\"font\", **font)\n",
    "plt.rc(\"xtick\", labelsize=16)\n",
    "plt.rc(\"ytick\", labelsize=16)\n",
    "plt.rc(\"axes\", labelsize=20)\n",
    "plt.rc(\"figure\", titlesize=22)\n",
    "plt.rc(\"legend\", fontsize=14)\n",
    "np.random.seed(46)\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d09b5f1-6ff7-4821-90fc-8ea9f494a5f1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data\n",
    "\n",
    "A dataset on used cars has been selected to analyze the proposed approach and provide examples. It contains information such as mileage and selling price on roughly 100,000 used cars from various popular brands and is available on [Kaggle](https://www.kaggle.com/datasets/adityadesai13/used-car-dataset-ford-and-mercedes?select=merc.csv) (as of July 2022)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae52fd70-59a5-4026-a346-9db86ae87afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    # Clean column names\n",
    "    df.columns = list(\n",
    "        map(\n",
    "            lambda x:\n",
    "            x.strip().lower().replace(\" \", \"\").replace(\"(Â£)\", \"\"),\n",
    "            df.columns\n",
    "        )\n",
    "    )\n",
    "    # Add brand (taken from filename)\n",
    "    df[\"brand\"] = filepath.split(\"/\")[-1].replace(\".csv\", \"\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99706871-702a-4373-80d7-a34706fa1ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually specify files instead of using\n",
    "# glob.glob(\"../data/[!unclean]*.csv\")\n",
    "# so that files are read in same order across systems.\n",
    "# In the future, use sorted() but this would affect\n",
    "# the progress made so far in the notebook\n",
    "# as results would change\n",
    "files = [\n",
    "    \"data/vauxhall.csv\",\n",
    "    \"data/bmw.csv\",\n",
    "    \"data/vw.csv\",\n",
    "    \"data/hyundi.csv\",\n",
    "    \"data/toyota.csv\",\n",
    "    \"data/ford.csv\",\n",
    "    \"data/focus.csv\",\n",
    "    \"data/skoda.csv\",\n",
    "    \"data/merc.csv\",\n",
    "]\n",
    "df = pd.concat(\n",
    "    [\n",
    "        # Disregard files whose name starts with \"unclean\"\n",
    "        load_df(i)\n",
    "        for i in files\n",
    "    ]\n",
    ")\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c806797f-f5ba-4883-831e-36dbcbc230a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df\n",
    "    .dtypes\n",
    "    .to_frame()\n",
    "    .reset_index()\n",
    "    .rename(\n",
    "        columns={\n",
    "            \"index\": \"column\",\n",
    "            0: \"data_type\"\n",
    "        }\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33465eaa-850c-4925-8d13-c819c729e991",
   "metadata": {},
   "source": [
    "Predictors such as tax and miles per gallon were disregarded to simplify the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef723d7-884f-420f-85d0-026120a2e03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features\n",
    "feature_cols = [\n",
    "    \"brand\",\n",
    "    \"model\",\n",
    "    \"year\",\n",
    "    \"mileage\",\n",
    "    \"transmission\",\n",
    "    \"fueltype\",\n",
    "    \"enginesize\",\n",
    "]\n",
    "# of which the following ones are of categorical type:\n",
    "categorical_feature_cols = [\"brand\", \"model\", \"fueltype\", \"transmission\"]\n",
    "\n",
    "numeric_cols = [i for i in feature_cols if i not in categorical_feature_cols]\n",
    "\n",
    "# Define the target of the model, i.e. what we want to predict\n",
    "target_col = \"price\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a269772d-4c59-4b6e-a1eb-c7429b15845d",
   "metadata": {},
   "source": [
    "## Clean data\n",
    "\n",
    "The first and only cleaning step involves removing entries with unrealistic attributes (cars built in the future and those with an engine capacity of zero liters). Once completed, obtaining high-level summaries of the considered attributes is now possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488335cc-b386-490c-abb9-5fd5a67dfee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    df.loc[lambda x: (x.enginesize > 0) & (x.year <= 2022)]\n",
    "    .copy()\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52b3045-fb96-42cb-a643-e88e4e711111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an ID to indexing precisely if random seed differs between systems\n",
    "df[\"ID\"] = df.index.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc141217-02ff-4f3f-a035-fe19ad49fa31",
   "metadata": {},
   "source": [
    "With this, let's obtain high level summaries of the considered attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af58221-f9bf-4849-bb27-d483f34e3ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4, (len(feature_cols) + 1) // 4, figsize=(20, 20))\n",
    "plt.suptitle(\"Car attributes considered\", fontsize=20)\n",
    "axes = axes.flatten()\n",
    "for i, c in enumerate(feature_cols + [target_col]):\n",
    "    if c in categorical_feature_cols:\n",
    "        if df[c].nunique() > 15:\n",
    "            tmp = df[c].value_counts(normalize=True).reset_index()\n",
    "            tmp.loc[15:, \"index\"] = \"Other\"\n",
    "            tmp = tmp.groupby(\"index\")[c].sum().sort_values(ascending=False)\n",
    "            tmp.plot(kind=\"bar\", ax=axes[i], color=\"darkviolet\", alpha=0.7)\n",
    "            axes[i].set_xlabel(\"Top 15 + all remaining in 'Other'\")\n",
    "        else:\n",
    "            df[c].value_counts(normalize=True).plot(\n",
    "                kind=\"bar\", ax=axes[i], color=\"darkviolet\", alpha=0.7\n",
    "            )\n",
    "    else:\n",
    "        sns.histplot(\n",
    "            data=df, x=c, ax=axes[i],\n",
    "            stat=\"probability\", color=\"darkviolet\",\n",
    "            alpha=0.7\n",
    "        )\n",
    "        axes[i].axvline(\n",
    "            df[c].mean(),\n",
    "            label=\"Average\",\n",
    "            linewidth=1,\n",
    "            linestyle=\"--\",\n",
    "            color=\"royalblue\",\n",
    "        )\n",
    "        axes[i].set_xlabel(\"\")\n",
    "        axes[i].legend()\n",
    "    axes[i].set_title(c.title(), fontsize=22)\n",
    "    axes[i].set_ylabel(\"Share\")\n",
    "    axes[i].yaxis.set_major_formatter(\n",
    "        ticker.FuncFormatter(lambda x, pos: f\"{x*100:,.0f}%\")\n",
    "    )\n",
    "    if c not in [\"year\", \"model\"]:\n",
    "        axes[i].xaxis.set_major_formatter(\n",
    "            ticker.FuncFormatter(lambda x, pos: f\"{x:,.0f}\")\n",
    "        )\n",
    "    if c == \"price\":\n",
    "        axes[i].xaxis.set_major_formatter(\n",
    "            ticker.FuncFormatter(lambda x, pos: f\"Â£{x:,.0f}\")\n",
    "        )\n",
    "        axes[i].tick_params(axis=\"x\", labelrotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../resources/eda_plot.png\", dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2eb043-cd00-4e47-ba8d-64b6365e1d0d",
   "metadata": {},
   "source": [
    "A few interesting insights stand out from these histograms:\n",
    "\n",
    "- Only a tiny fraction of cars are EVs or hybrids.\n",
    "- Most of the cars are sold at prices close to $Â£16,000$.\n",
    "- Ford cars are the most popular ones, especially considering that the brand âfocusâ is most likely referring to a car model produced by Ford, namely Ford Focus.\n",
    "\n",
    "While a more thorough analysis may reveal more, data exploration and preparation are outside the scope of this blog post and therefore have not been explored."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe15da49-ab2e-4911-9455-5d99e100ae9e",
   "metadata": {},
   "source": [
    "# Task\n",
    "\n",
    "With a grasp of the data, let us imagine ourselves in the shoes of a car dealer, who must decide, based on some relevant car properties, at what price to sell it. \n",
    "\n",
    "To support car dealers, we will provide them with an automated tool that suggests an indicative, initial price point based on the prices of historically sold cars. \n",
    "\n",
    "Such a tool is assumed to simulate how car dealers naturally set prices, by looking at prices that similar cars fetched."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5ba25f-43ca-4a27-8af2-053d29d8947f",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "To solve the defined task, a Catboost Regressor is chosen. However, really any ensemble method of decision trees (regressors or classifiers) could be used.\n",
    "\n",
    "For modeling, neardefault hyperparameters are used (only slightly modifying the number of estimators and size of leaves), and the data is randomly split in a 80-20 fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d82a85-00ac-4067-bbf2-4abc9c4f2d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate model\n",
    "model = CatBoostRegressor(random_seed=46, n_estimators=50, min_data_in_leaf=4)\n",
    "\n",
    "# Define name of prediction column\n",
    "predicted_col = \"predicted_\" + target_col\n",
    "\n",
    "# Set categorical features to categorical dtype\n",
    "df[categorical_feature_cols] = (\n",
    "    df[categorical_feature_cols].fillna(\"None\").astype(\"category\")\n",
    ")\n",
    "\n",
    "# Define splits\n",
    "train_idx, test_idx = train_test_split(\n",
    "    np.arange(len(df)), test_size=0.2, random_state=46\n",
    ")\n",
    "\n",
    "# Manually modify the splits because there are 2 cars\n",
    "# we would like to have in our test set\n",
    "# As these will be used as examples throughout the notebook\n",
    "# In case the random seed has a different\n",
    "# effect across platforms and machines\n",
    "# we can make sure that at least these 2 examples are the same\n",
    "car_ids_in_test = [20844, 13284]\n",
    "for car_id in car_ids_in_test:\n",
    "    if car_id not in test_idx:\n",
    "        print(f\"Modifying train/test split to include car ID {car_id}\")\n",
    "        test_idx = np.append(test_idx, car_id)\n",
    "        car_idx_train = np.where(train_idx == car_id)[0][0]\n",
    "        train_idx = np.delete(train_idx, car_idx_train)\n",
    "assert all([i in test_idx and i not in train_idx for i in car_ids_in_test])\n",
    "\n",
    "# Create data splits\n",
    "df_train = df.loc[train_idx].reset_index(drop=True).copy()\n",
    "df_test = df.loc[test_idx].reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a620f7-4b3e-4882-be66-7f8ef10e7e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "model.fit(\n",
    "    df_train[feature_cols],\n",
    "    df_train[target_col],\n",
    "    cat_features=categorical_feature_cols,\n",
    "    verbose=False,\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2389644c-6ec5-4748-a18a-cd8b6b046b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "for ds in [df_train, df_test]:\n",
    "    # Make predictions\n",
    "    ds[predicted_col] = model.predict(ds[feature_cols]).astype(int)\n",
    "    # Evaluate model\n",
    "    diff = ds[predicted_col] - ds[target_col]\n",
    "    ds[\"absolute_percentage_error\"] = diff.abs() / ds[target_col]\n",
    "    ds[\"relative_percentage_error\"] = diff / ds[target_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71491ca-3674-49d9-adf6-25622eca8be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    pd.concat((df_train.assign(split=\"train\"), df_test.assign(split=\"test\")))\n",
    "    .groupby(\"split\")\n",
    "    .agg(\n",
    "        mepe=(\"relative_percentage_error\", \"mean\"),\n",
    "        mape=(\"absolute_percentage_error\", \"mean\"),\n",
    "    )\n",
    "    .rename(\n",
    "        columns={\n",
    "            \"mepe\": \"Mean Relative Percentage Error\",\n",
    "            \"mape\": \"Mean Absolute Percentage Error\",\n",
    "        }\n",
    "    )\n",
    "    .applymap(lambda x: \"{:.1f}%\".format(x * 100))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bd8772-61f9-4de4-be77-7eda47d8d77d",
   "metadata": {},
   "source": [
    "The results above suggest the model performs reasonably well as, on average, it is only off by about 9% and tends to slightly over-predict the actual price (ca. 1% higher on average). \n",
    "\n",
    "Again, the goal here is not to develop the best performing model, but to ensure that it accomplishes the task with reasonable accuracy and that no overfitting occurs.\n",
    "\n",
    "Now that the model is trained, we can turn to state-of-the-art XAI techniques to make this black box more transparent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95c2ff3-58be-46b9-b378-c1d31a10dba5",
   "metadata": {},
   "source": [
    "# SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d2ed27-fc2f-4b32-8045-559be886a670",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_explainer = shap.TreeExplainer(model)\n",
    "shap_values = model_explainer(df_train[feature_cols])\n",
    "shap_values_test = model_explainer(df_test[feature_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84abebf9-81b7-48c8-8aa0-563cc547701d",
   "metadata": {},
   "source": [
    "Lime and SHAP are popular tools and, in many aspects, very similar. However, as the objective is to describe their common shortcomings, we will only focus on SHAP.\n",
    "\n",
    "SHAP can be used to obtain global insights into what features are most important and how they affect the predictions of a model.\n",
    "A concrete example would be obtaining the most critical features in terms of their average, absolute contributions to the model predictions.\n",
    "\n",
    "These predictions are often visualized using the built-in bar plot provided by SHAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e71e455-fed9-46aa-8f47-f4d7db1ff551",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.bar(shap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db611a17-1e65-44b8-b01e-e8df48d98c6c",
   "metadata": {},
   "source": [
    "Here, we can see that the engine size is by far the most relevant attribute when it comes to predicting car sale prices (with an average absolute impact of $\\approx Â£ 3,100$).\n",
    "\n",
    "Similarly, the beeswarm plot is useful to gauge not only the impact but also the sign of the contribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79fde42-2ab2-41ec-a0b7-20eb5393847d",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.beeswarm(shap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23abebcf-d330-4b9c-a681-b5255bb5b0fd",
   "metadata": {},
   "source": [
    "We can see that larger engine sizes (higher feature values, coloured in red) are related to higher predictions of sales prices (larger SHAP value).\n",
    "\n",
    "However, SHAP can also be used to obtain local explanations, i.e. explain the prediction of a particular instance, in our case a specific car we want to sell.\n",
    "\n",
    "Consider the following car for which we would like to determine the sales price for:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443687f8-fd5d-47ba-a305-01c8b2513c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a specific car with nice properties\n",
    "# for the sake of explaining how SHAP works\n",
    "car_to_explain_id = 20844\n",
    "example = df_test.query(f\"ID == {car_to_explain_id}\")[feature_cols]\n",
    "car_to_explain_idx = example.index[0]\n",
    "display(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7acde8e-cacb-46c9-861b-290db514e430",
   "metadata": {},
   "source": [
    "To get some insight into what affected the decision of the model, we can use the popular waterfall plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107bbd62-f97d-4759-9034-1c97b0d02c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.waterfall(shap_values_test[car_to_explain_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc14175-c5de-4ebb-9aeb-cb9be8eeba48",
   "metadata": {},
   "source": [
    "The plot shows the influence of selected features on the model prediction $f(x)$ for this specific car compared to the average prediction of all the cars in the training set $E[f(X)]$. Concretely, the model expects the average car to sell for $E[f(X)] \\approx Â£16,000$, while it predicts the selected car to sell for $f(x) \\approx Â£31,000$.<br/>\n",
    "A major reason that the model estimates the price of this car to be higher than that of the average is its $3$ litre, engine size, which translates into an increase above the mean expected sales price of $\\approx Â£12,500$.\n",
    "\n",
    "For car dealers however, the above explanations might not be very intuitive. They may not be interested in an explanation referencing the average car as it is not clear what the average car is without some insights into the dataset (see histograms in the Data section).<br/>\n",
    "Instead, it might be more relevant to understand how this prediction compares to similar cars sold in the past.\n",
    "\n",
    "To address this, we propose an approach to find similar training instances, i.e., those on which the model mainly bases its prediction.\n",
    "This way, car dealers can directly gauge the observed sales prices of similar cars, allowing them to put those into perspective and understand from where a predicted price stems.\n",
    "\n",
    "To this end, we introduce our example-based XAI technique named `LeafSim`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0813915-2d95-4de5-8afb-8fed4239e635",
   "metadata": {},
   "source": [
    "# LeafSim\n",
    "\n",
    "The proposed approach works only for decision trees and their ensemble derivatives, such as the popular Catboost algorithm.<br/>\n",
    "It leverages how such models are built to easily extract similar training instances for a given prediction we wish to explain.\n",
    "\n",
    "More specifically, each decision tree partitions the feature space through a series of binary questions.<br/>\n",
    "Each question is a branch. If a branch is not followed by another question, it is colloquially known as a leaf.<br/>\n",
    "Leaves are determined based on a chosen convergence criteria, such as RMSE for regression.<br/>\n",
    "Further, leaves provide predictions based on the training observations ending up in them.<br/>\n",
    "For regression, for example, these predictions are based on the average target value of those observations.\n",
    "\n",
    "This entire process is succinctly summarised in the following image from the [Encyclopedia of Machine Learning](https://link.springer.com/referenceworkentry/10.1007/978-0-387-30164-8_711), which visualises the case of regression using two predictor variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9030c4cd-543c-4c3d-9ef2-c257f90d7996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Image to display so it can be rendered in PDF when running\n",
    "# jupyter nbconvert --to webpdf --no-input notebook.ipynb\n",
    "# As opposed to:\n",
    "# <div>\n",
    "# <img src=\"images/decision_tree_regression.jpg\" width=\"600\"/>\n",
    "# </div>\n",
    "Image(filename=\"../resources/decision_tree_regression.jpg\", width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb58fda-e6bf-45bd-8fdb-14fee3c0a0d0",
   "metadata": {},
   "source": [
    "Here, the squared boxes refer to the leaves; their associated prediction value is simply the average $Y$ value of the observations falling into them.\n",
    "\n",
    "Based on this understanding of how decision trees are built and how they operate, it becomes evident that two data points ending up in the same leaf have very similar features (by satisfying the same conditions as imposed by binary questions that create branches) and similar targets (by satisfying the convergence criteria of leaves).\n",
    "\n",
    "Thus, we define similar instances as those that end up most often in the same leaf across all decision trees in an ensemble. Hence the name `LeafSim`. This approach is loosely related to [[1]](https://arxiv.org/pdf/1802.06640.pdf), which introduced LeafRefit to rank training instances based on the change in loss for a given test instance.\n",
    "\n",
    "Visually, `LeafSim` can be represented as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229186e3-5e28-4847-a179-e0f82b55c870",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename=\"../resources/leaf_index.png\", width=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54a1168-f686-4d31-b759-1429022851a3",
   "metadata": {},
   "source": [
    "Here, $x_{test}$ is the instance for which we want to explain the prediction and $X_{train}$ are all the instances in the training data, with $x_i$ being an individual observation (in our case a car).\n",
    "\n",
    "Similarity can thus be measured using the [Hamming distance](https://en.wikipedia.org/wiki/Hamming_distance).<br/>\n",
    "Briefly, the Hamming distance captures, for two strings of equal length, the number of positions for which the symbols differ. <br/>\n",
    "This allows us to measure the number of times leaf indices differ across trees.<br/>\n",
    "By dividing this number by the number of trees and subtracting it from $1$, we can transform it into a bounded similarity metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ff9241-502c-4184-86b7-c08980da5671",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "\n",
    "In the case of Catboost, `LeafSim` can easily leverage the built-in function [`calc_leaf_indexes`](https://catboost.ai/en/docs/concepts/python-reference_catboost_calc_leaf_indexes) to obtain the leaf indices.\n",
    "For `XGBoost` the equivalent would be [`apply`](https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBRegressor.apply).\n",
    "\n",
    "For those who wish to learn more about the implementation of LeafSim and access the code used in this blog post, please refer to the [Renku repository](https://renkulab.io/projects/lucas.chizzali/leafsim/files/blob/notebooks/LeafSim.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763c1783-cdc7-48e3-a30a-ea2b7f9c06c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since this notebook is available on a hosting platform\n",
    "# and can be executed there, let's reduce the computational complexity\n",
    "# by only finding similar training observations for some cars in the test set\n",
    "df_test_subset = pd.concat(\n",
    "    (\n",
    "        df_test.loc[lambda x: ~x.ID.isin(car_ids_in_test)].sample(frac=0.2),\n",
    "        df_test.loc[lambda x: x.ID.isin(car_ids_in_test)],\n",
    "    )\n",
    ").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17909f48-3f1b-476d-8289-37d3a2e097de",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaf_sim = LeafSim(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f0913f-ce11-4885-b5f0-9cd7bbb45894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10\n",
    "top_n_ids, top_n_similarities = leaf_sim.generate_explanations(\n",
    "    X_train=df_train[feature_cols].values,\n",
    "    X_to_explain=df_test_subset[feature_cols].values,\n",
    "    top_n=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf960d3d-77a3-49a6-b3a8-49d11fb3bf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 50 (for more advanced analysis)\n",
    "_, top_50_similarities, similarities = leaf_sim.generate_explanations(\n",
    "    X_train=df_train[feature_cols].values,\n",
    "    X_to_explain=df_test_subset[feature_cols].values,\n",
    "    top_n=50,\n",
    "    return_all_similarities=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cc5e9e-3252-4e89-ac60-d9a12e3e43b0",
   "metadata": {},
   "source": [
    "# Examples\n",
    "\n",
    "To understand how these `LeafSim` scores can be utilized, let us start by analysing two concrete examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a719ebbc-d715-4c06-acee-8d561021eb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the most important features according to SHAP as a list\n",
    "# These features will be used when showing similar instances\n",
    "# Measure importance in absolute terms\n",
    "abs_mean_shap_values = np.abs(shap_values.values).mean(0)\n",
    "df_feature_importance = pd.DataFrame(\n",
    "    list(zip(feature_cols, abs_mean_shap_values)),\n",
    "    columns=[\"feature_name\", \"feature_importance\"],\n",
    ").sort_values(by=[\"feature_importance\"], ascending=False)\n",
    "# Restrict ourselves to the top 8 most influential features\n",
    "top_n_features = list(df_feature_importance.feature_name.values[:8])\n",
    "\n",
    "# Define the formatting of some columns\n",
    "formatting = {\n",
    "    target_col: lambda x: f\"Â£ {x:,.0f}\",\n",
    "    predicted_col: lambda x: f\"Â£ {x:,.0f}\",\n",
    "    \"similarity\": lambda x: f\"{x*100:.0f}%\",\n",
    "    \"enginesize\": lambda x: f\"{x:.1f}\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0feccc6-596c-4304-8025-c5b29709ba84",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_top_sim = top_50_similarities.mean(axis=1)\n",
    "df_test_subset[\"avg_top_sim\"] = avg_top_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b84f5c-661b-4799-9d3b-080e73ec514d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the utils function into another function to\n",
    "# reduce the number of parameters we have to pass\n",
    "_get_similarity_table = lambda car_id: get_similarity_table(\n",
    "    df_train,\n",
    "    df_test_subset,\n",
    "    top_n_ids,\n",
    "    top_n_similarities,\n",
    "    car_id,\n",
    "    top_n_features,\n",
    "    formatting,\n",
    ")\n",
    "\n",
    "_get_similarity_plots = lambda car_id: get_similarity_plots(\n",
    "    df_train, df_test_subset, similarities, car_id, avg_top_sim\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d24b17-2bff-41e3-8e71-afbbeed831f0",
   "metadata": {},
   "source": [
    "## Example 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c53c61c-9996-4cad-b518-576e8b31c68a",
   "metadata": {},
   "source": [
    "As a first example, we will pick the BMW that we previously used to explain SHAP, and as this is a common brand and model, we expect the model to make a reasonably accurate prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1e3c0b-885b-4e5b-92c0-86bb5fefa924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the same car from the SHAP example\n",
    "car_to_explain_id = 20844  # index in df_test\n",
    "example = df_test_subset.query(f\"ID == {car_to_explain_id}\")[\n",
    "    top_n_features + [predicted_col, target_col]\n",
    "]\n",
    "car_to_explain_idx = example.index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8df5da1-2b01-4842-bdfe-3829f8c76c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will not be nicely rendered in Quarto\n",
    "# See https://github.com/quarto-dev/quarto-cli/discussions/1716\n",
    "# display(example.style.format(formatting))\n",
    "# Hence don't format prices:\n",
    "(\n",
    "    example.assign(\n",
    "        predicted_price=lambda x: x.predicted_price.apply(\"Â£{:,}\".format),\n",
    "        price=lambda x: x.price.apply(\"Â£{:,}\".format),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5e8d93-1e8c-43b6-a869-ca607ebb56ac",
   "metadata": {},
   "source": [
    "The table above describes the attributes of our selected car alongside the model prediction and the car's actual sale price. \n",
    "\n",
    "We can see the model provides an accurate, but we would like to understand which cars in the training set this prediction is mostly based on.\n",
    "\n",
    "For simplicity, we restrict ourselves to the Top 10 cars with the highest `LeafSim` score, i.e., the ten cars that appear most often in the same leaf as the car we want to explain. This choice depends on how fast `LeafSim` scores drop in the ranking. If there is minimal change, then showing a large subset of the most similar observations will lead to redundant information.\n",
    "\n",
    "\n",
    "The Top 10 cars are described in the table below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653637e8-5552-4dff-94f5-f8899f618be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(_get_similarity_table(car_to_explain_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6dc1f6-18bf-4551-8ac7-38eab12540e7",
   "metadata": {},
   "source": [
    "The colours in the table represent how similar the cars are to the car we want to explain.<br/>\n",
    "The lighter the shade of red of a cell, the more similar the two cars are in this respect,<br/>\n",
    "while the darker the shade of blue in the similarity column is, the more similar the two cars.\n",
    "\n",
    "We can see that for the most important features, the provided cars are indeed very similar.<br/>\n",
    "Their sale prices are also very much in line with the selected car's predicted sale price.\n",
    "\n",
    "In the above table, we have restricted the data to the Top 10.<br/>\n",
    "However, we could also investigate the relationship between price and the `LeafSim` score across the entire training set, as is shown in the following plot on the left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b7b546-a7d1-4af0-a641-8a1b5de8baa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "_get_similarity_plots(car_to_explain_idx)\n",
    "plt.savefig(\"../resources/example_1.png\", dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735da0bf-22e7-410c-9f86-a36c3a59bb2d",
   "metadata": {},
   "source": [
    "The prices of cars with a higher `LeafSim` score vary less and are generally closer to the prediction of the car we want to explain.\n",
    "\n",
    "To put the `LeafSim` score of this particular car in perspective, the plot on the right shows the distribution of the average LeafSim score for the Top 50 training instances.<br/>\n",
    "We can see that this car has several similar training instances (the mean `LeafSim` score of the Top 50 is roughly 80%). <br/>\n",
    "This indicates the model is basing its prediction on very relevant training samples and therefore the prediction can be assumed to be fairly accurate.\n",
    "\n",
    "\n",
    "The choice of 50 here is somewhat arbitrary and data-dependent.<br/>\n",
    "It should be a value that equals the number of observations we can typically expect to be highly similar.<br/>\n",
    "Predictions with few similar training instances should stand out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be78f332-a3dd-40aa-9eae-c22f9bc7cb2d",
   "metadata": {},
   "source": [
    "## Example 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f528da-0bd2-42f6-b534-7221a60a6b74",
   "metadata": {},
   "source": [
    "As a second example, let us pick an electric car. As these are drastically under-represented in the dataset, the model will perform poorly on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b6dd7e-a5f4-405c-96b0-e3ff47947c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set error rate by fueltype\n",
    "(\n",
    "    df_test.groupby([\"fueltype\"])\n",
    "    .agg(mean_absolute_percentage_error=(\"absolute_percentage_error\", \"mean\"))\n",
    "    .sort_values(by=\"mean_absolute_percentage_error\")\n",
    "    .rename(\n",
    "        columns={\n",
    "            \"mean_absolute_percentage_error\": \"Mean Absolute Percentage Error\"\n",
    "        }\n",
    "    )\n",
    "    .applymap(lambda x: \"{:.1f}%\".format(x * 100))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5849dd-3e83-4c5e-b635-3b713608eefc",
   "metadata": {},
   "source": [
    "Indeed, the error rate of EVs is roughly 3x larger than for any other fuel type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a43bb28-7ed5-4895-bd41-4ae1c4630ad0",
   "metadata": {},
   "source": [
    "Let us pick a specific electric car:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0300974-e590-4d96-942a-4e02c78ae38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick an electric car\n",
    "car_to_explain_id = 13284  # index in df_test\n",
    "example = df_test_subset.query(f\"ID == {car_to_explain_id}\")[\n",
    "    top_n_features + [predicted_col, target_col]\n",
    "]\n",
    "car_to_explain_idx = example.index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00680707-87c3-4a97-8622-f4e1875d994d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(example.style.format(formatting))\n",
    "(\n",
    "    example.assign(\n",
    "        predicted_price=lambda x: x.predicted_price.apply(\"Â£{:,}\".format),\n",
    "        price=lambda x: x.price.apply(\"Â£{:,}\".format),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b06cc7-2e08-4166-bdd8-6bcac9a71cc5",
   "metadata": {},
   "source": [
    "We already see that the predicted price is quite different from the actual price.\n",
    "\n",
    "In a real world setting however, we do not know the actual price.<br/>\n",
    "This is because the tool will be used by car dealers to set the selling price and thus the car has not been sold yet.<br/>\n",
    "\n",
    "To better understand what this prediction is based on and whether one can fully trust it, we can again turn to the most similar training examples as identified by `LeafSim`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4c4dbe-00c4-4bd2-a2ce-b16738b7a6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(_get_similarity_table(car_to_explain_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e61adbc-d5a3-4e62-94a5-6799e0195e8b",
   "metadata": {},
   "source": [
    "In this case, we can clearly see that the most similar training instances are not very close to the predicted car.\n",
    "\n",
    "The `LeafSim` scores are low (below 50%) and crucial attributes vary considerably, such as brand, model, and fuel type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd91449-5836-4f5f-8fa1-6a6c00c0f7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "_get_similarity_plots(car_to_explain_idx)\n",
    "plt.savefig(\"../resources/example_2.png\", dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023f765e-5ec2-451d-b3f3-66a978852222",
   "metadata": {},
   "source": [
    "We also see from the above plot on the left, that there is a lot of price variation, even for the most similar cars.<br/>\n",
    "From the plot on the right we can clearly see that the average `LeafSim` scores of the most related training instances are very low.\n",
    "\n",
    "Therefore, car dealers should lower their confidence in the model prediction for this particular car."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb96dd26-b010-47eb-bdcb-911fd47d95d9",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this blog post, we addressed the interpretability gap of popular XAI techniques, such as SHAP and Lime, regarding their use by people with limited knowledge in Machine Learning.\n",
    "\n",
    "Specifically, an example-based approach to explain decision-tree based ensemble methods was proposed, identifying the most relevant instances in the training set on which the model bases a particular prediction.\n",
    "This measure of relevance is expressed using the `LeafSim` score, which captures how often observations end up in the same leaf.\n",
    "\n",
    "Mainly through a qualitative assessment of selected examples, we demonstrated that the `LeafSim` score indeed reflects similarity between observations.<br/>\n",
    "Furthermore, the model tends to predict observations with higher LeafSim scores more accurately.<br/>\n",
    "Therefore, end-users can use such scores to adjust their confidence in model predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcab33c3-31dd-4fdf-bb52-9f1943bbea81",
   "metadata": {},
   "source": [
    "# About the Author\n",
    "\n",
    "[Lucas Chizzali](https://datascience.ch/team_member/lucas-chizzali-data-scientist/) is a Data Scientist at the Swiss Data Science Center (SDSC).\n",
    "\n",
    "Lucas joined the SDSCâs industry cell as a Data Scientist in November 2020, having previously worked in data related roles at the New York State Attorney and at Ericsson. He holds a BSc in Economics from Bocconi University, a MSc in Urban Science and Informatics from New York University as well as a MSc in Machine Learning from KTH Royal Institute of Technology. Over the course of his academic and professional career he has worked on a variety of topics, from computer vision tasks for automated driving to financial fraud detection to generating data driven insights to inform urban policy decisions.\n",
    "\n",
    "`LeafSim` was developed while working at Richemont."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebb7008-487e-4dc4-8a99-6ab7c80b6367",
   "metadata": {},
   "source": [
    "# About the SDSC\n",
    "\n",
    "The [Swiss Data Science Center (SDSC)](https://datascience.ch/) is a joint venture between EPFL and ETH Zurich. Its mission is to accelerate the adoption of data science and machine learning techniques within academic disciplines of the ETH domain, the Swiss academic community at large, and the industrial sector. In particular, it addresses the gap between those who create data, those who develop data analytics and systems, and those who could potentially extract value from it. The center comprises a multi-disciplinary team of data and computer scientists as well as experts in selected domains with offices in ZÃ¼rich, Lausanne, and Villigen.<br/>www.datascience.ch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e34f70-909a-4a56-93bf-72d8615ec9ab",
   "metadata": {},
   "source": [
    "# Acknowledgements\n",
    "\n",
    "The author would like to thank  colleagues at Richemont and SDSC, namely Elvire Bouvier, Francesco Calabrese and Valerio Rossetti for their support in developing `LeafSim` and for their valuable feedback in writing this blog post."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d79908e-8341-40cb-bb3d-0a6098d6cc02",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[1] Sharchilev, Boris, Yury Ustinovskiy, Pavel Serdyukov, and Maarten Rijke. \"Finding influential training samples for gradient boosted decision trees.\" In International Conference on Machine Learning, pp. 4577-4585. PMLR, 2018."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ef74e6-30a1-49b9-9e2d-d2eaa5f06c6c",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "\n",
    "## Technical Considerations\n",
    "\n",
    "Each tree has the same weight in `LeafSim`, which may not be the most faithful approach depending on the Machine Learning method we would like to explain.<br/>\n",
    "Empirical results for gradient boosting show that giving equal weights to all trees provides useful and insightful results.<br/>\n",
    "One way to verify this is to analyse the correlation between error and the `LeafSim` score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91398ed2-9c80-41a2-86a6-03565e7639f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
